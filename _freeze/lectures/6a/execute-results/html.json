{
  "hash": "c7b913b460edd3cfd9ed60c6dccc81e7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: |\n  Convolutional \\\n  Neural Networks\nsubtitle: \"Image Classification with CIFAR\"\nformat:\n  revealjs:\n    width: 1200\n    scrollable: false\n    include-in-header: \"math.html\"\n    footer: m408.inqs.info/lectures/6a\n    theme: [default, styles.scss]\n    preview-links: true\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 48\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: false\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n## R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"tidyverse\")\n# install.packages(\"torch\")\n# install.packages(\"luz\")\n# install.packages(\"torchvision\")\n\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\nlibrary(tidyverse)\nlibrary(torchvision) # for datasets and image transformation\n```\n:::\n\n\n## Python Data\n\n``` python\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n```\n\n# Image Classification\n\n## Image Classification\n\nImage classification is the process of having a computer analyzse an image and classify it with a selected category.\n\n## Image Classification\n\nWhat is this a picture of?\n\n::: fragment\n![](img/squirrel.jpg){fig-align=\"center\" width=\"500\"}\n:::\n\n## Image Classification\n\nHow did you know it was a squirrel?\n\n# Convolutional Neural Networks\n\n## Convolutional Neural Networks\n\nConvolutional Neural Networks became popular in the 2010's with the success of image classification.\n\n::: fragment\nThe idea is to mimic how a human mind will classify an image. (From an old understanding of neurobiology.)\n:::\n\n::: fragment\nWith the use of convolutional filters, a convolutional neural networks is trained by using a set of images that have been previously classfied.\n:::\n\n::: fragment\nOnce the network is trained, new images can be classified.\n:::\n\n## CNN\n\n![Credit: ISLR2](img/islr2/Chapter10/10_6.jpg){fig-align=\"center\"}\n\n## Architecture\n\n![](https://www.embedded.com/wp-content/uploads/2023/06/23026adi-cnn1_f03_thumb.jpg){fig-align=\"center\"}\n\n\n# Input Data\n\n## Input Data\n\n# Convolutional Filter\n\n## Convolution Filter\n\nA Convolution Filter will highlight certain features of an image.\n\n::: fragment\nThe matching features will contain a large value.\n:::\n\n::: fragment\nDismatching features will contain a smaller value.\n:::\n\n## Convolution Filter\n\n::: panel-tabset\n## Image\n\n$$\n\\left(\n\\begin{array}{ccc}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l \n\\end{array}\n\\right)\n$$\n\n## Filter\n\n$$\n\\left(\n\\begin{array}{cc}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{array}\n\\right)\n$$\n\n## Both\n\n$$\n\\left(\n\\begin{array}{ccc}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l \n\\end{array}\n\\right)\n*\n\\left(\n\\begin{array}{cc}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{array}\n\\right)\n$$\n\n## Results\n\n$$\n\\left(\n\\begin{array}{cc}\na\\alpha + b\\beta + d\\gamma + w\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\nd\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\ng\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n\\end{array}\n\\right)\n$$\n:::\n\n## Convolutional Layers\n\nConvolution layers are a set of filters in a hidden layers. We can have $K$ layers that an image is passed through.\n\n## Pooling Layers\n\nThe act of summarizing a large matrix to a smaller matrix.\n\n## Max Pool\n\n$$\n\\left[\n\\begin{array}{cccc}\n1 & 3 & 9 & 5 \\\\\n6 & 2 & 3 & 4 \\\\\n1 & 0 & 6 & 4 \\\\\n8 & 4 & 2 & 7\n\\end{array}\n\\right] \\rightarrow\n\\left[\n\\begin{array}{cc}\n6 & 9 \\\\\n8 & 7\n\\end{array}\n\\right]\n$$\n\n\n## CIFAR Data\n\n## Convolve Image\n\n## Flattening\n\nOnce the images has been pooled to a select pixels or features. The images are flattened to a set of inputs.\n\n::: fragment\nThese inputs are used to a traditional neural network to classify an image.\n:::\n\n## Training\n\nThe CNN is trained by supplying a set of pre-classified images.\n\n::: fragment\nThe parameters in the convolution filters are estimated using standard techniques.\n:::\n\n# R CIFAR Data Analysis\n\n## CIFAR Data\n\n## Torch Packages in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\nlibrary(torchdatasets) # for datasets we are going to use\nlibrary(zeallot)\ntorch_manual_seed(13)\n```\n:::\n\n\n## CIFAR-10\n\n",
    "supporting": [
      "6a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}