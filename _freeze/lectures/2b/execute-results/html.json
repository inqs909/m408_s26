{
  "hash": "a16afc55a29be0ed547127ed110e127c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Torch\"\nsubtitle: \"Tensors\"\nformat:\n  revealjs:\n    width: 1200\n    scrollable: false\n    include-in-header: \"math.html\"\n    footer: m408.inqs.info/lectures/1a\n    theme: [default, styles.scss]\n    preview-links: true\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 48\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n## R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(torch)\nlibrary(mvtnorm)\n```\n:::\n\n\n# Tensors\n\n## What are Tensors?\n\n\n:::: {.columns}\n::: {.column}\n\n::: {.callout-note icon=false}\n\n### Mathematics\n\nA [tensor](https://en.wikipedia.org/wiki/Tensor) describes a relationship between algebraic objects.\n\n:::\n\n:::\n::: {.column}\n\n::: {.callout-tip icon=false} \n\n### Torch\n\nAn n-dimensional array with optimized operations.\n\n:::\n:::\n:::: \n\n## Creating Tensors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1 <- torch_tensor(matrix(1:9, ncol = 3))\nt2 <- torch_tensor(1:3)\nt3 <- torch_tensor(array(1:24, dim = c(4, 3, 2)))\n```\n:::\n\n\n## Tensor Operations\n\n## Dimensions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 3 3\n```\n\n\n:::\n\n```{.r .cell-code}\nt2$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 3\n```\n\n\n:::\n\n```{.r .cell-code}\nt3$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 4 3 2\n```\n\n\n:::\n:::\n\n\n## Type\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$dtype\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_Long\n```\n\n\n:::\n\n```{.r .cell-code}\nt2$dtype\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_Long\n```\n\n\n:::\n\n```{.r .cell-code}\nt3$dtype\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_Long\n```\n\n\n:::\n:::\n\n\n\n## Changing Type\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$to(dtype = torch_float())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1  4  7\n#>  2  5  8\n#>  3  6  9\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nt2$to(dtype = torch_float())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1\n#>  2\n#>  3\n#> [ CPUFloatType{3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nt3$to(dtype = torch_float())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#> (1,.,.) = \n#>    1  13\n#>    5  17\n#>    9  21\n#> \n#> (2,.,.) = \n#>    2  14\n#>    6  18\n#>   10  22\n#> \n#> (3,.,.) = \n#>    3  15\n#>    7  19\n#>   11  23\n#> \n#> (4,.,.) = \n#>    4  16\n#>    8  20\n#>   12  24\n#> [ CPUFloatType{4,3,2} ]\n```\n\n\n:::\n:::\n\n\n## Set Type\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1 <- torch_tensor(matrix(1:9, ncol = 3), dtype = torch_float())\nt2 <- torch_tensor(1:3, dtype = torch_float())\nt3 <- torch_tensor(array(1:24, dim = c(4, 3, 2)), dtype = torch_float())\n```\n:::\n\n\n## Device\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$device\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_device(type='cpu')\n```\n\n\n:::\n\n```{.r .cell-code}\nt2$device\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_device(type='cpu')\n```\n\n\n:::\n\n```{.r .cell-code}\nt3$device\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_device(type='cpu')\n```\n\n\n:::\n:::\n\n\n## Set Device\n\n```r\nt1 <- torch_tensor(matrix(1:9, ncol = 3), device = \"cuda\")\nt2 <- torch_tensor(1:3, device = \"cuda\")\nt3 <- torch_tensor(array(1:24, dim = c(4, 3, 2)), device = \"cuda\")\n```\n::: {.callout-note}\nUse if cuda is installed (Nvidia users)\n:::\n\n# Vector Operations\n\n## Vectors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1 <- torch_tensor(1:6)\nt2 <- torch_tensor(7:12)\nt3 <- torch_tensor(1:3)\nt1; t2; t3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1\n#>  2\n#>  3\n#>  4\n#>  5\n#>  6\n#> [ CPULongType{6} ]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>   7\n#>   8\n#>   9\n#>  10\n#>  11\n#>  12\n#> [ CPULongType{6} ]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1\n#>  2\n#>  3\n#> [ CPULongType{3} ]\n```\n\n\n:::\n:::\n\n\n\n## Addition/Subtraction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$add(t2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>   8\n#>  10\n#>  12\n#>  14\n#>  16\n#>  18\n#> [ CPULongType{6} ]\n```\n\n\n:::\n:::\n\n\n## Scalar Multiplication\n\n\n::: {.cell}\n\n```{.r .cell-code}\n10*t1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  10\n#>  20\n#>  30\n#>  40\n#>  50\n#>  60\n#> [ CPUFloatType{6} ]\n```\n\n\n:::\n:::\n\n\n## Elementwise Multiplication\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$multiply(t2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>   7\n#>  16\n#>  27\n#>  40\n#>  55\n#>  72\n#> [ CPULongType{6} ]\n```\n\n\n:::\n:::\n\n\n## Dot Product\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$dot(t2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#> 217\n#> [ CPULongType{} ]\n```\n\n\n:::\n:::\n\n\n# Matrix Operations\n\n## Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1 <- torch_tensor(matrix(1:9, ncol = 3), dtype = torch_float())\nt2 <- torch_tensor(matrix(rpois(9, 3), ncol = 3), dtype = torch_float())\nt3 <- torch_tensor(matrix(1:3, nrow = 3), dtype = torch_float())\n\nt1; t2; t3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1  4  7\n#>  2  5  8\n#>  3  6  9\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  2  4  4\n#>  4  3  3\n#>  3  2  4\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1\n#>  2\n#>  3\n#> [ CPUFloatType{3,1} ]\n```\n\n\n:::\n:::\n\n\n\n## Transpose\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$t()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1  2  3\n#>  4  5  6\n#>  7  8  9\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nt2$t()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  2  4  3\n#>  4  3  2\n#>  4  3  4\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nt3$t()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1  2  3\n#> [ CPUFloatType{1,3} ]\n```\n\n\n:::\n:::\n\n\n\n## Addition/Subtraction\n\nMore information on matrix arithmetic can be found [here](https://shainarace.github.io/LinearAlgebra/mult.html#matrix-addition-subtraction-and-scalar-multiplication).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$add(t2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>   3   8  11\n#>   6   8  11\n#>   6   8  13\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nt1$subtract(t2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#> -1  0  3\n#> -2  2  5\n#>  0  4  5\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n:::\n\n\n## Scalar Multiplication\n\n\n::: {.cell}\n\n```{.r .cell-code}\n10*t1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  10  40  70\n#>  20  50  80\n#>  30  60  90\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n:::\n\n\n## Element-wise Multiplications\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1; t2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1  4  7\n#>  2  5  8\n#>  3  6  9\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  2  4  4\n#>  4  3  3\n#>  3  2  4\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nt1$multiply(t2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>   2  16  28\n#>   8  15  24\n#>   9  12  36\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n:::\n\n\n## Matrix Multiplication\n\nMore information on matrix multiplication can be found [here](https://shainarace.github.io/LinearAlgebra/mult.html#matrix-multiplication).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$matmul(t2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  39  30  44\n#>  48  39  55\n#>  57  48  66\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nt2$matmul(t1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  22  52  82\n#>  19  49  79\n#>  19  46  73\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\nt3$t()$matmul(t1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  14  32  50\n#> [ CPUFloatType{1,3} ]\n```\n\n\n:::\n:::\n\n\n## Determinant\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$det()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#> 0\n#> [ CPUFloatType{} ]\n```\n\n\n:::\n:::\n\n\n## Inverse\n\n::: {.cell}\n\n```{.r .cell-code}\nt2$inverse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#> -0.3000  0.4000  0.0000\n#>  0.3500  0.2000 -0.5000\n#>  0.0500 -0.4000  0.5000\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n:::\n\n\n\n## Diagonal Elements\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1$diag()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1\n#>  5\n#>  9\n#> [ CPUFloatType{3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\ntorch_diag(1:3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1  0  0\n#>  0  2  0\n#>  0  0  3\n#> [ CPULongType{3,3} ]\n```\n\n\n:::\n:::\n\n\n\n## Matrix of 0/1\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntorch_zeros(3,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  0  0  0\n#>  0  0  0\n#>  0  0  0\n#> [ CPUFloatType{3,3} ]\n```\n\n\n:::\n\n```{.r .cell-code}\ntorch_ones(2,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1  1  1\n#>  1  1  1\n#> [ CPUFloatType{2,3} ]\n```\n\n\n:::\n:::\n\n\n## Identity Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntorch_eye(n = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>  1  0  0  0\n#>  0  1  0  0\n#>  0  0  1  0\n#>  0  0  0  1\n#> [ CPUFloatType{4,4} ]\n```\n\n\n:::\n:::\n\n\n# Linear Regression\n\n## Linear Regression\n\n$$\nY_i = 3.85 + 12.3 X_1 - 9.7 X_2 + \\varepsilon_i\n$$\n\n$$\n\\varepsilon \\sim N(0, 2.4)\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta <- c(3.85, 12.3, -9.7)\nx <- rmvnorm(500, c(3, 8))\nxx <- cbind(1, x)\ny <- xx%*%beta + rnorm(500, sd = sqrt(2.4))\n```\n:::\n\n\n## OLS Estimator\n\n$$\n\\hat\\bbeta = (\\bX^\\mrT\\bX)\\inv\\bX^\\mrT\\bY\n$$\n\n::: {.columns}\n::: {.column}\n$$\n\\bX =\\left(\n\\begin{array}{ccc}\n1 & X_{1,1} & X_{2,1} \\\\\n1 & X_{1,2} & X_{2,2} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & X_{1,500} & X_{2,500} \\\\\n\\end{array}\n\\right)\n$$\n\n:::\n::: {.column}\n$$\n\\bY =\\left(\n\\begin{array}{ccc}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_{500}\n\\end{array}\n\\right)\n$$\n\n:::\n:::\n\n## Using R\n\n$$\n\\hat\\bbeta = (\\bX^\\mrT\\bX)\\inv\\bX^\\mrT\\bY\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = y ~ x)\n#> \n#> Coefficients:\n#> (Intercept)           x1           x2  \n#>       3.826       12.258       -9.690\n```\n\n\n:::\n\n```{.r .cell-code}\nsolve(t(xx)%*%xx)%*%t(xx)%*%y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>           [,1]\n#> [1,]  3.825633\n#> [2,] 12.257655\n#> [3,] -9.690359\n```\n\n\n:::\n:::\n\n\n## Using torch\n\n$$\n\\hat\\bbeta = (\\bX^\\mrT\\bX)\\inv\\bX^\\mrT\\bY\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxxt <- torch_tensor(xx)\nyt <- torch_tensor(y)\n\nxxt$t()$matmul(xxt)$inverse()$matmul(xxt$t())$matmul(yt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> torch_tensor\n#>   3.8235\n#>  12.2576\n#>  -9.6900\n#> [ CPUFloatType{3,1} ]\n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "2b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}