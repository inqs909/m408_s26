{
  "hash": "8dbba6f9b026fa8d1364b8d9dadf661b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Neural Networks\"\nsubtitle: \"Classification\"\nformat:\n  revealjs:\n    width: 1200\n    scrollable: false\n    include-in-header: \"math.html\"\n    footer: m408.inqs.info/lectures/5a\n    theme: [default, styles.scss]\n    preview-links: true\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 48\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: false\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n## R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"torch\")\n# install.packages(\"luz\")\n\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\nlibrary(tidyverse)\ntorch_manual_seed(13)\n```\n:::\n\n\n## Data in Python\n\n```python\nimport plotnine\nfrom plotnine.data import penguins\nclean_penguins = (\n    penguins\n        .dropna()\n        .drop(columns=[\"year\"])\n)\n```\n\n## Penguins Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins <- penguins |> tidyr::drop_na()\ndplyr::glimpse(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 333\n#> Columns: 8\n#> $ species     <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ad…\n#> $ island      <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Tor…\n#> $ bill_len    <dbl> 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6, 34.6…\n#> $ bill_dep    <dbl> 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2, 21.1…\n#> $ flipper_len <int> 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 185, 195…\n#> $ body_mass   <int> 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800, 4400…\n#> $ sex         <fct> male, female, female, female, male, female, male, female, …\n#> $ year        <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n```\n\n\n:::\n:::\n\n\n\n# Classification\n\n## Classification\n\nClassification in statistical learning terms indicates predicting a categorical random variable.\n\n## Example\n\nLet's say we are interested in predicting the penguin species: Gentoo, Chinstrap, and Adelie. \n\n::: {.fragment}\n$$\nY  = \\left\\{\n\\begin{array}{c}\nGentoo \\\\\nChinstrap \\\\\nAdelie\n\\end{array}\n\\right.\n$$\n:::\n\n\n## Common Methods\n\n- Naive Bayes Classifier\n\n- Tree-based Methods\n\n- Support Vector Machines\n\n- Logistic/Multinomial regression\n\n- Discriminant Analysis\n\n::: {.fragment}\nWe are primarily going to use a logistic model approach.\n:::\n\n\n## Model\n\n$$\nY = f(\\boldsymbol X; \\boldsymbol \\theta)\n$$\n\n- $\\boldsymbol X$: a vector of predictor variables\n- $\\boldsymbol \\theta$: a vector of parameters\n\n::: {.fragment}\nHow do we model an outcome which is categorical, not a number?\n:::\n\n\n\n## Modeling Probabilities\n\nTo model categorical data, we will model the probability of oberving each specific category. \n\n## Neural Netork\n\nLet \n\n$$\n\\bmcH(\\bX; \\btheta_G)\n$$\n\n$$\n\\bmcH(\\bX; \\btheta_A)\n$$\n\n$$\n\\bmcH(\\bX; \\btheta_C)\n$$\n\nbe three functions related to neural networks.\n\n\n## Unnormalized Probability \n\n$$\n\\tilde P\\left(Y = Gentoo\\right) = exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_G)\\right\\}\n$$\n\n$$\n\\tilde P\\left(Y = Adelie\\right) = exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_A)\\right\\}\n$$\n\n$$\n\\tilde P\\left(Y = Chinstrap\\right) = exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_C)\\right\\}\n$$\n\n## Normalized Probability (softmax) {.smaller}\n\n\n$$\nf_G(\\bX) = P\\left(Y = Gentoo\\right) = \\frac{exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_G)\\right\\}}{exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_G)\\right\\} + exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_A)\\right\\} + exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_C)\\right\\}}\n$$\n\n$$\nf_A(\\bX) = P\\left(Y = Adelie\\right) = \\frac{exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_A)\\right\\}}{exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_G)\\right\\} + exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_A)\\right\\} + exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_C)\\right\\}}\n$$\n\n$$\nf_C(\\bX) = P\\left(Y = Chinstrap\\right) = \\frac{exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_C)\\right\\}}{exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_G)\\right\\} + exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_A)\\right\\} + exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_C)\\right\\}}\n$$\n\n## For **J** Categories\n\n$$\nf_j(\\bX) = P\\left(Y = j\\right) = \\frac{exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_j)\\right\\}}{\\sum^J_{j=1}exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_j)\\right\\}}\n$$\n\n\n# Buliding Classifiers\n\n## Neural Networks\n\nNeural Networks are capable of classifying data by fitting \n\n## Multilayer Neural Network\n\n![](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png){fig-align=\"center\"}\n\n## Hidden Layer 1\n\nWith $p$ predictors of $X$:\n\n$$\nh^{(1)}_k(X) = H^{(1)}_k  = g\\left\\{\\alpha_{k0} + \\sum^p_{j=1}\\alpha_{kj}X_{j}\\right\\}\n$$\nfor $k = 1, \\cdots, K$ nodes.\n\n## Hidden Layer 2\n\n$$\nh^{(2)}_l(X) = H^{(2)}_l = g\\left\\{\\beta_{l0} + \\sum^K_{k=1}\\beta_{lk}H^{(1)}_{k}\\right\\}\n$$\nfor $l = 1, \\cdots, L$ nodes.\n\n\n## Hidden Layer 3 +\n\n$$\nh^{(3)}_m(X) = H^{(3)}_l = g\\left\\{\\gamma_{m0} + \\sum^L_{l=1}\\gamma_{ml}H^{(2)}_{l}\\right\\}\n$$\nfor $m = 1, \\cdots, M$ nodes.\n\n## Pre-Output Layer\n\n$$\n\\bmcH(X) = \\beta_{0} + \\sum^M_{m=1}\\beta_{m}H^{(3)}_m\n$$\n\n## Output Layer\n\n$$\nf_j(\\bX) = P\\left(Y = j\\right) = \\frac{exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_j)\\right\\}}{\\sum^k_{j=1}exp\\left\\{\\bmcH(\\boldsymbol X; \\boldsymbol \\theta_j)\\right\\}}\n$$\n\nfor all **J** functions (categories).\n\n\n\n# Log-likelihood Function\n\n## Fitting Model\n\nFitting a neural network is the process of taking input data ($X$), finding the numerical values for the paramters that will maximizing (or in this case minimizing) the log-likelihood function, and finding the maximum likelihood estimator via gradient descent.\n\n## Log-likelihood Function\n\n$$\n\\ell (\\btheta) = - \\sum^n_{i=1}\\sum^k_{j=1} Y_{ij}\\log\\left\\{f_j(\\bX_i;\\btheta)\\right\\}\n$$\n\n\n$$\nY_{ij} = \\left\\{\n\\begin{array}{cc}\n1 & j\\mrth\\ \\mathrm{category} \\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n$$\n\n\n::: {.fragment}\nAlso known as *cross-entropy*.\n:::\n\n\n## Penguins Data\n\n$$\n\\ell (\\btheta) = - \\sum^{333}_{i=1}\\sum^3_{j=1} Y_{ij}\\log\\left\\{f_j(\\bX_i;\\btheta)\\right\\}\n$$\n\n\n# Single-Layer Neural Network\n\n## Single-Layer Neural Network\n\nWe will predict penguin species with a single-layer containing 10 nodes, and predict each species (3). We will use a ReLU activation function.\n\n## Penguin Data\n\n::: {.panel-tabset}\n\n### Train/Test/Evaluation\n  \n\n::: {.cell}\n\n```{.r .cell-code}\npenguins <- penguins |> drop_na()\ntraining <- penguins |> slice_sample(prop = .8)\npre <- penguins |> anti_join(training)\nvalidate <- pre |> slice_sample(prop =  0.5)\ntesting <- pre |> anti_join(validate)\n```\n:::\n\n\n\n\n###  Training\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtraining <- training |> \n  model.matrix(species ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYtraining <- training |> \n  select(species) |> \n  as.matrix() |>\n  as.factor() |> \n  torch_tensor()\n```\n:::\n\n\n### Validate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXvalidate <- validate |> \n  model.matrix(species ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYvalidate <- validate |> \n  select(species) |> \n  as.matrix() |>\n  as.factor() |> \n  torch_tensor()\n```\n:::\n\n\n### Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtesting <- testing |> \n  model.matrix(species ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYtesting <- testing |> \n  select(species) |> \n  as.matrix() |>\n  as.factor() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n\n:::\n\n## Model Description\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn <- nn_module(\n  initialize = function(input_size) {\n    self$hidden1 <- nn_linear(in_features = input_size, \n                              out_features = 10)\n    self$output <- nn_linear(in_features = 10, \n                             out_features = 3)\n        \n    self$activation <- nn_relu()\n  },\n  forward = function(x) {\n    x |>  \n      self$hidden1() |>  \n      self$activation() |>  \n      self$output()\n  }\n)\n```\n:::\n\n\n\n## Optimizer Set Up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn <- modelnn |> \n  setup(\n    loss = nn_cross_entropy_loss(), # Used for numerical counts\n    optimizer = optim_rmsprop\n  ) |>\n  set_hparams(input_size = ncol(Xtraining))\n```\n:::\n\n\n\n## Fit a Model\n\n::: {.panel-tabset}\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- modelnn |> \n  fit(\n    data = list(Xtraining, Ytraining),\n    epochs = 50, # Can think as number of iterations\n    valid_data = list(Xvalidate, Yvalidate)\n  )\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(fitted)\n```\n:::\n\n\n\n### Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- fitted |>  \n  predict(Xtesting) |>  \n  torch_argmax(dim = 2) |>   \n  as_array() \n\nmean(as.numeric(Ytesting)==res)\n```\n:::\n\n:::\n\n\n# Multi-Layer Neural Network\n\n## Multi-Layer Neural Network\n\nBuild a multi-layer neural network that will predict `species` with the remaining predictors and the following components:\n\n- 3 Hidden Layers \n    - 20 nodes\n    - 10 nodes\n    - 5 nodes\n- output is 3 seperate functions\n- Use ReLU activation\n\n\n## Model Description\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn2 <- nn_module(\n  initialize = function(input_size) {\n    self$hidden1 <- nn_linear(in_features = input_size, \n                              out_features = 20)\n    self$hidden2 <- nn_linear(in_features = 20, \n                              out_features = 10)\n    self$hidden3 <- nn_linear(in_features = 10, \n                              out_features = 5)\n\n    self$output <- nn_linear(in_features = 5, \n                             out_features = 3)\n        \n    self$activation <- nn_relu()\n  },\n  forward = function(x) {\n    x |>  \n      self$hidden1() |>  \n      self$activation() |>  \n      \n      self$hidden2() |>  \n      self$activation() |>  \n\n      self$hidden3() |>  \n      self$activation() |>  \n\n      self$output()\n  }\n)\n```\n:::\n\n\n\n## Optimizer Set Up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn2 <- modelnn2 |> \n  setup(\n    loss = nn_cross_entropy_loss(), # Used for numerical counts\n    optimizer = optim_rmsprop\n  ) |>\n  set_hparams(input_size = ncol(Xtraining))\n```\n:::\n\n\n\n## Fit a Model\n\n::: {.panel-tabset}\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted2 <- modelnn2 |> \n  fit(\n    data = list(Xtraining, Ytraining),\n    epochs = 50, # Can think as number of iterations\n    valid_data = list(Xvalidate, Yvalidate)\n  )\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fitted2)\n```\n:::\n\n\n\n### Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres2 <- fitted2 |>  \n  predict(Xtesting) |>  \n  torch_argmax(dim = 2) |>   \n  as_array() \nmean(as.numeric(Ytesting)==res)\nmean(as.numeric(Ytesting)==res2)\n```\n:::\n\n:::\n\n\n# Generalizing Neural Networks\n\n## Overfitting\n\nOverfitting is the concept where the model becomes to well in predicting the data. The fear is that the model can only predict the data it was trained on, and not any other data point. Therfore, it cannot be deployed.\n\n::: {.fragment}\nTherefore, we deploy three method to generalize the model:\n\n- Get more data.\n- Dropout\n- Regularization\n- Early Stopping\n:::\n\n## More Data\n\nOne of the best ways to prevent over fitting is get more data that is representative.\n\n::: {.fragment}\nThis ensures that the model is more generalizable.\n:::\n\n## Dropout\n\n**Dropout** is the process where we randomly makes nodes into zero. This ensures that connections do not get to dependent on a critical node. It will need to rely on the other nodes to get better as well. \n\n## Dropout\n\n::: panel-tabset\n\n### Model Description\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn3 <- nn_module(\n  initialize = function(input_size) {\n    self$hidden1 <- nn_linear(in_features = input_size, \n                              out_features = 20)\n    self$hidden2 <- nn_linear(in_features = 20, \n                              out_features = 10)\n    self$hidden3 <- nn_linear(in_features = 10, \n                              out_features = 5)\n\n    self$output <- nn_linear(in_features = 5, \n                             out_features = 3)\n        \n    self$activation <- nn_relu()\n    self$drop20 <- nn_dropout(p = 0.2)\n    self$drop30 <- nn_dropout(p = 0.3)\n\n  },\n  forward = function(x) {\n    x |>  \n      self$hidden1() |>  \n      self$activation() |>  \n      self$drop20() |> \n      \n      self$hidden2() |>  \n      self$activation() |>  \n      self$drop30() |> \n      \n      self$hidden3() |>  \n      self$activation() |>  \n      \n      self$output()\n  }\n)\n```\n:::\n\n\n### Optimizer Set Up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn3 <- modelnn3 |> \n  setup(\n    loss = nn_cross_entropy_loss(), # Used for numerical counts\n    optimizer = optim_rmsprop\n  ) |>\n  set_hparams(input_size = ncol(Xtraining))\n```\n:::\n\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted3 <- modelnn3 |> \n  fit(\n    data = list(Xtraining, Ytraining),\n    epochs = 50, # Can think as number of iterations\n    valid_data = list(Xvalidate, Yvalidate)\n  )\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fitted3)\n```\n:::\n\n\n\n### Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres3 <- fitted3 |>  \n  predict(Xtesting) |>  \n  torch_argmax(dim = 2) |>   \n  as_array() \nmean(as.numeric(Ytesting)==res)\nmean(as.numeric(Ytesting)==res2)\nmean(as.numeric(Ytesting)==res3)\n```\n:::\n\n\n:::\n\n## Regularization\n\nRegularization is the process where we add a penalty term to all the parameters and when fitting the data. This will ensure that the certain parameters will go to zero.\n\n::: {.fragment}\n$$\n\\ell (\\btheta) = - \\sum^n_{i=1}\\sum^k_{j=1} Y_{ij}\\log\\left\\{f_j(\\bX_i;\\btheta)\\right\\} - \\lambda \\sum_r |\\btheta_r|\n$$\n\n:::\n\n## Regularization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn3 <- modelnn3 |> \n  setup(\n    loss = nn_cross_entropy_loss(), # Used for numerical counts\n    optimizer = optim_rmsprop\n  ) |>\n  set_hparams(input_size = ncol(Xtraining)) |> \n  set_opt_hparams(weight_decay = 0.000001)\n```\n:::\n\n\n## Early Stopping\n\nWe can reduce the number of epochs to train our model and so the parameters do not converge. This will allow the model to not fully explain the data, and in return, be more generalizable to new data.\n\n## Early Stopping \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted3 <- modelnn3 |> \n  fit(\n    data = list(Xtraining, Ytraining),\n    epochs = 5, # Can think as number of iterations\n    valid_data = list(Xvalidate, Yvalidate)\n  )\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}