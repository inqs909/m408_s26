{
  "hash": "4ef2142e06f6e15b8d6cf6cec9f2adae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Neural Networks\"\nsubtitle: \"with Torch\"\nformat:\n  revealjs:\n    width: 1200\n    scrollable: false\n    include-in-header: \"math.html\"\n    footer: m408.inqs.info/lectures/3a\n    theme: [default, styles.scss]\n    preview-links: true\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 48\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: false\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n## R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"torch\")\n# install.packages(\"luz\")\n\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\ntorch_manual_seed(13)\n```\n:::\n\n\n## Data in Python\n\n```python\nimport plotnine\nfrom plotnine.data import penguins\nclean_penguins = (\n    penguins\n        .dropna()\n        .drop(columns=[\"year\"])\n)\n```\n\n\n# Machine Learning\n\n## What is Machine Learning?\n\nMachine Learning (ML) is the process of characterizing mathematical models, with the help of data, to predict outcomes of interest.\n\n\n## Machine Learning Model\n\n$$\nY = f(\\boldsymbol X; \\boldsymbol \\theta)\n$$\n\n- $Y$: is an outcome of interest that we are trying to predict\n- $\\boldsymbol X$: A vector of data points used to predict the outcome\n- $f$: An unknown function that predicts the outcome\n- $\\boldsymbol \\theta$: A vector of parameters to used to define the model \n\n## What is going on?\n\n- Data helps buil in a model\n- Uses mathematical models to predict outcomes\n- Uses probability theory to model randomness and loss\n- Uses numerical algorithms to:\n  - Find numerical values of parameters that minimize randomness\n  - Model unknown and nonlinear relationships\n\n## Main Types of Machine Learning\n\n::: {.columns}\n::: {.column}\n\n### Supervised Learning\n\n- Data includes **inputs** and **outputs**\n- ML Models learns to map inputs â†’ outputs\n\n\n:::\n::: {.column}\n\n### Unsupervised Learning\n\n- Data has **no labels**\n- Model discovers hidden patterns\n\n\n:::\n:::\n\n\n\n# Linear Regression\n\n## Linear Regression\n\nLinear Regression is a tool to predict continuous random variables that are known to follow a Normal Distribution, with a set of known predictor variables. \n\n## Simple Linear Regression\n\nSimple linear regression will model the association between one predictor variable and an outcome:\n\n$$\n\\hat Y = \\beta_0 + \\beta_1 X \n$$\n\n-   $\\beta_0$: Intercept term\n\n-   $\\beta_1$: Slope term\n\n\n## MLR\n\nMultivariable linear regression models are used when more than one explanatory variable is used to explain the outcome of interest.\n\n## Additional Continuous Variable\n\nTo fit additional variable to the model, we will only need to add it to the model:\n\n$$\n\\hat Y = \\beta_0 +\\beta_1 X_{1} + \\beta_2 X_{2} \n$$\n\n## Categorical Variable\n\nA categorical variable can be included in a model, but a reference category must be specified.\n\n## Fitting a model with categorical variables\n\nTo fit a model with categorical variables, we must utilize dummy (binary) variables that indicate which category is being referenced. We use $C-1$ dummy variables where $C$ indicates the number of categories. When coded correctly, each category will be represented by a combination of dummy variables.\n\n## Example\n\nIf we have 4 categories, we will need 3 dummy variables:\n\n|         | Cat 1 | Cat 2 | Cat 3 | Cat 4 |\n|---------|-------|-------|-------|-------|\n| Dummy 1 | 1     | 0     | 0     | 0     |\n| Dummy 2 | 0     | 1     | 0     | 0     |\n| Dummy 3 | 0     | 0     | 1     | 0     |\n\nWhich one is the reference category?\n\n## Model with categorical variables\n\nFitting an additional variable with 4 Categories \n\n$$\n\\hat Y = \\beta_0 +\\beta_1 X_{1} + \\beta_2 X_{2} + \\beta_3 D_{1} + \\beta_4 D_{2} + \\beta_5  D_{3}\n$$\n\n\n\n## Sums of Squared Errors\n\nWe find the values of $\\beta_0, \\cdots, \\beta_5$ that minimizes the following function for $i$ data points:\n\n$$\nRSS = \\sum^n_{i=1}(Y_i-\\hat Y_i)^2\n$$\n\n## Matrix Formulation\n\n\n$$\nY_i = \\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta + \\epsilon_i\n$$\n\n-   $Y_i$: Outcome Variable\n\n-   $\\boldsymbol X_i$: Predictors\n\n-   $\\boldsymbol \\beta$: Coefficients\n\n-   $\\epsilon_i$: error term\n\n## Matrix Data Formulation\n\n\n$$\n\\boldsymbol\\beta = (\\boldsymbol X^\\mathrm T \\boldsymbol X)^{-1} \\boldsymbol X^\\mathrm T \\boldsymbol Y\n$$\n\n\n# Neural Networks\n\n## Neural Networks\n\nNeural networks are a type of machine learning algorithm that are designed to mimic the function of the human brain. They consist of interconnected nodes or \"neurons\" that process information and generate outputs based on the inputs they receive.\n\n## Uses\n\nNeural networks are typically used for tasks such as image recognition, natural language processing, and prediction. They are capable of learning from data and improving their performance over time, which makes them well-suited for complex and dynamic problems.\n\n## Neural Network\n\n![](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png){fig-align=\"center\"}\n\n## Neural Network Composition\n\n- Inputs: A set of characteristics in the data that we use to predict the outcome of interest\n- Outputs: A set of variables (may be one) we wish to predict \n- Hidden Layers: A set of functions that will transform the data such that it can better predict the outputs\n    - Each hidden layer will has nodes that indicates the transformation\n\n\n## Single Layer Neural Network\n\n![](https://www.oreilly.com/api/v2/epubs/9781789808452/files/assets/290136cc-48f2-47b1-bb95-ffdb625b987d.png){fig-align=\"center\"}\n\n\n## Model Setup\n\n$$\nY = f(\\boldsymbol X; \\boldsymbol \\theta)\n$$\n\n- $\\boldsymbol X$: a vector of predictor variables\n- $\\boldsymbol \\theta$: a vector of parameters ($\\boldsymbol \\beta, \\boldsymbol \\alpha$)\n\n## Single Layer Neural Networks\n\nA single layer neural networks can be formulated as linear function:\n\n$$\nf(\\boldsymbol X; \\boldsymbol \\theta) = \\beta_0 + \\sum^K_{k=1}\\beta_kh_k(\\boldsymbol X)\n$$\n\nWhere $X$ is a vector of inputs of length $p$ and $K$ is the number of nodes (neurons), $\\beta_j$ are parameters \n\n$$\nh_k(\\boldsymbol X) = g\\left(\\alpha_{k0} + \\sum^p_{l=1}\\alpha_{kl}X_{l}\\right)\n$$\n\nwith $g(\\cdot)$ being a nonlinear activation function and $\\alpha_{kl}$ are the weights (parameters).\n\n## Fitting a Neural Network\n\nFitting a neural network is the process of taking input data ($X$), finding the numerical values for the paramters that will minimize the following loss function, mean squared errors (MSE):\n\n$$\n\\frac{1}{n}\\sum^n_{i-1}\\left\\{Y_i-f(\\boldsymbol X; \\boldsymbol \\theta)\\right\\}^2\n$$\n\n# Other Topics\n\n## Nonlinear (Activations) Function $g(\\cdot)$\n\nActivation functions are used to create a nonlinear affect within the neural network. Common activation functions are\n\n-   Sigmoidal: $g(z) = \\frac{1}{1+e^{-z}}$ (nn_sigmoidal)\n\n-   ReLU (rectified linear unit): $g(z) = (z)_+ = zI(z\\geq0)$ (nn_relu)\n\n-   Hyperbolic Tangent: $g(z) = \\frac{\\sinh(z)}{\\cosh(z)} = \\frac{\\exp(z) - \\exp(-z)} {\\exp(z) + \\exp(-z)}$ (nn_tanh)\n\nOtherwise, the neural network is just an overparameterized linear model.\n\n\n## Optimizer\n\nThe optimizer is the mathematical algorithm used to find the numerical values for the parameters $\\beta_j$ and $\\alpha_{kl}$.\n\n::: {.fragment}\nThe most basic algorithm used in gradient descent.\n:::\n\n\n# Single-Layer Neural Network in R\n\n## Penguin Data\n\n::: {.panel-tabset}\n\n## Description\n\nBuild a single-layer neural network that will predict `body_mass` with the remaining predictors. The hidden layer will contain 20 nodes, and the activation functions will be ReLU.\n\n## Data Prep\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\npenguins <- penguins |> drop_na()\npx <- penguins |>\n  model.matrix(body_mass ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\npy <- penguins |> \n  select(body_mass) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n  \n  \n:::\n\n\n## Model Description\n\n::: {.panel-tabset}\n### Overall\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nmodnn <- nn_module(\n  initialize = function(input_size) {\n    self$hidden <- nn_linear(input_size, 20)\n    self$activation <- nn_relu()\n    self$output <- nn_linear(20, 1)\n  },\n  forward = function(x) {\n    x |> \n      self$hidden() |>  \n      self$activation() |>  \n      self$output()\n  }\n)\n```\n:::\n\n  \n### Initialize\n\nCreates the functions needed to describe the details of each network.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitialize = function(input_size) {\n    self$hidden <- nn_linear(input_size, 50)\n    self$activation <- nn_relu()\n    self$output <- nn_linear(50, 1)\n  }\n```\n:::\n\n\n\n### Forward\n\nModels the neural network. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nforward = function(x) {\n    x |> \n      self$hidden() |>  \n      self$activation() |>  \n      self$output()\n  }\n```\n:::\n\n\n\n:::\n\n\n\n## Optimizer Set Up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodnn <- modnn |> \n  setup(\n    loss = nn_mse_loss(), # Used for numerical counts\n    optimizer = optim_rmsprop\n  ) |>\n  set_hparams(input_size = ncol(px))\n```\n:::\n\n\n\n## Fit a Model\n\n::: {.panel-tabset}\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- modnn |> \n  fit(\n    data = list(px, py),\n    epochs = 200 # Can think as number of iterations\n  )\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(fitted)\n```\n:::\n\n\n:::\n\n\n# Training/Validating/Testing Data\n\n\n## Error Rate\n\nWhen creating a model, we are interested in determining how effective the model will be in predicting a new data point, ie not in our training data. \n\n::: {.fragment}\nThe error rate is a metric to determine how often will future data points be when using our model.\n:::\n\n::: {.fragment}\nThe problem is how can we get future data to validate our model?\n:::\n\n## Training/Validating/Testing Data\n\nThe Training/Validating/Testing Data set is a way to take the original data set and split into 3 seperate data sets: training, validating, and testing.\n\n::: {.panel-tabset}\n\n### Training\n\nThis is data used to create the model. \n\n### Validating\n\nThis is data used to evaluate the data during it's creation. It is evaluate at each Iteration (Epoch)\n\n### Testing\n\nThis is data used to test the final model and compute the error rate.\n\n:::\n\n\n## Training Error Rate\n\nTraining Error Rate is the error rate of the data used to create the model of interest. It describes how well the model predicts the data used to construct it.\n\n## Test Error Rate\n\nTest Error Rate is the error rate of predicting a new data point using the current established model.\n\n\n\n\n## Penguin Data\n\n::: {.panel-tabset}\n\n### Train/Test/Evaluation\n  \n\n::: {.cell}\n\n```{.r .cell-code}\npenguins <- penguins |> drop_na()\ntraining <- penguins |> slice_sample(prop = .8)\npre <- penguins |> anti_join(training)\nvalidate <- pre |> slice_sample(prop =  0.5)\ntesting <- pre |> anti_join(validate)\n```\n:::\n\n\n\n\n###  Training\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtraining <- training |> \n  model.matrix(body_mass ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYtraining <- training |> \n  select(body_mass) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n\n### Validate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXvalidate <- validate |> \n  model.matrix(body_mass ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYvalidate <- validate |> \n  select(body_mass) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n\n### Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtesting <- testing |> \n  model.matrix(body_mass ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYtesting <- testing |> \n  select(body_mass) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n\n:::\n\n## Model Description\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\nmodnn <- nn_module(\n  initialize = function(input_size) {\n    self$hidden <- nn_linear(input_size, 20)\n    self$activation <- nn_relu()\n    self$output <- nn_linear(20, 1)\n  },\n  forward = function(x) {\n    x |> \n      self$hidden() |>  \n      self$activation() |>  \n      self$output()\n  }\n)\n```\n:::\n\n\n\n## Optimizer Set Up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodnn <- modnn |> \n  setup(\n    loss = nn_mse_loss(), # Used for numerical counts\n    optimizer = optim_rmsprop\n  ) |>\n  set_hparams(input_size = ncol(px))\n```\n:::\n\n\n\n## Fit a Model\n\n::: {.panel-tabset}\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- modnn |> \n  fit(\n    data = list(Xtraining, Ytraining),\n    epochs = 200, # Can think as number of iterations\n    valid_data = list(Xvalidate, Yvalidate)\n  )\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(fitted)\n```\n:::\n\n\n:::\n\n## Testing Model\n\n::: {.panel-tabset}\n\n### Prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnpred <- predict(fitted, Xtesting)\n```\n:::\n\n\n### MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(abs(as.matrix(Ytesting) - as.matrix(npred)))\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(as.matrix(Ytesting), as.matrix(npred),\n     xlab = \"Truth\",\n     ylab = \"Predicted\")\n```\n:::\n\n\n:::\n\n\n",
    "supporting": [
      "3a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}