{
  "hash": "67495b6d7730c97610f17b5c9b58bcd7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Neural Networks\"\nsubtitle: \"Multi-layer Neural networks\"\nformat:\n  revealjs:\n    width: 1200\n    scrollable: false\n    include-in-header: \"math.html\"\n    footer: m408.inqs.info/lectures/3a\n    theme: [default, styles.scss]\n    preview-links: true\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 48\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: false\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n## R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"torch\")\n# install.packages(\"luz\")\n\nlibrary(torch)\nlibrary(luz) # high-level interface for torch\ntorch_manual_seed(13)\n```\n:::\n\n\n## Data in Python\n\n```python\nimport plotnine\nfrom plotnine.data import penguins\nclean_penguins = (\n    penguins\n        .dropna()\n        .drop(columns=[\"year\"])\n)\n```\n\n\n# Multilayer Neural Networks\n\n\n## Neural Network\n\n![](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png){fig-align=\"center\"}\n\n## Neural Network Composition\n\n- Inputs: A set of characteristics in the data that we use to predict the outcome of interest\n- Outputs: A set of variables (may be one) we wish to predict \n- Hidden Layers: A set of functions that will transform the data such that it can better predict the outputs\n    - Each hidden layer will has nodes that indicates the transformation\n\n\n## Multilayer Neural Network\n\nMultilayer Neural Networks create multiple hidden layers where each layer feeds into each other which will create a final outcome.\n\n## Model Setup\n\n$$\nY = f(\\boldsymbol X; \\boldsymbol \\theta)\n$$\n\n- $\\boldsymbol X$: a vector of predictor variables\n- $\\boldsymbol \\theta$: a vector of parameters ( $\\boldsymbol \\alpha, \\boldsymbol \\beta, \\boldsymbol \\gamma, \\boldsymbol \\delta$)\n\n\n\n## Multilayer Neural Network\n\n![](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20230602113310/Neural-Networks-Architecture.png){fig-align=\"center\"}\n\n## Hidden Layer 1\n\nWith $p$ predictors of $X$:\n\n$$\nh^{(1)}_k(X) = H^{(1)}_k  = g\\left\\{\\alpha_{k0} + \\sum^p_{j=1}\\alpha_{kj}X_{j}\\right\\}\n$$\nfor $k = 1, \\cdots, K$ nodes.\n\n## Hidden Layer 2\n\n$$\nh^{(2)}_l(X) = H^{(2)}_l = g\\left\\{\\beta_{l0} + \\sum^K_{k=1}\\beta_{lk}H^{(1)}_{k}\\right\\}\n$$\nfor $l = 1, \\cdots, L$ nodes.\n\n\n## Hidden Layer 3 +\n\n$$\nh^{(3)}_m(X) = H^{(3)}_l = g\\left\\{\\gamma_{m0} + \\sum^L_{l=1}\\gamma_{ml}H^{(2)}_{l}\\right\\}\n$$\nfor $m = 1, \\cdots, M$ nodes.\n\n## Output Layer\n\n$$\nf(X) = \\beta_{0} + \\sum^M_{m=1}\\beta_{m}H^{(3)}_m\n$$\n\n\n\n## Fitting a Neural Network\n\nFitting a neural network is the process of taking input data ($X$), finding the numerical values for the paramters that will minimize the following loss function, mean squared errors (MSE):\n\n$$\n\\frac{1}{n}\\sum^n_{i-1}\\left\\{Y_i-f(\\boldsymbol X; \\boldsymbol \\theta)\\right\\}^2\n$$\n\n# Other Topics\n\n## Nonlinear (Activations) Function $g(\\cdot)$\n\nActivation functions are used to create a nonlinear affect within the neural network. Common activation functions are\n\n-   Sigmoidal: $g(z) = \\frac{1}{1+e^{-z}}$ (nn_sigmoidal)\n\n-   ReLU (rectified linear unit): $g(z) = (z)_+ = zI(z\\geq0)$ (nn_relu)\n\n-   Hyperbolic Tangent: $g(z) = \\frac{\\sinh(z)}{\\cosh(z)} = \\frac{\\exp(z) - \\exp(-z)} {\\exp(z) + \\exp(-z)}$ (nn_tanh)\n\nOtherwise, the neural network is just an overparameterized linear model.\n\n\n## Optimizer\n\nThe optimizer is the mathematical algorithm used to find the numerical values for the parameters $\\beta_j$ and $\\alpha_{kl}$.\n\n::: {.fragment}\nThe most basic algorithm used in gradient descent.\n:::\n\n\n# Multi-Layer Neural Network in R\n\n## Penguin Data\n\n::: {.panel-tabset}\n\n### Description\n\nBuild a single-layer neural network that will predict `body_mass` with the remaining predictors. The hidden layer will contain 20 nodes, and the activation functions will be ReLU.\n\n### Data Prep\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\npenguins <- penguins |> drop_na()\npx <- penguins |>\n  model.matrix(body_mass ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\npy <- penguins |> \n  select(body_mass) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n  \n  \n:::\n\n## Model Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn <- nn_module(\n  initialize = function(input_size) {\n    self$hidden1 <- nn_linear(in_features = input_size, \n                              out_features = 20)\n    self$hidden2 <- nn_linear(in_features = 20, \n                              out_features = 10)\n    self$hidden3 <- nn_linear(in_features = 10, \n                              out_features = 5)\n\n    self$output <- nn_linear(in_features = 5, \n                             out_features = 1)\n        \n    self$activation <- nn_relu()\n  },\n  forward = function(x) {\n    x |>  \n      self$hidden1() |>  \n      self$activation() |>  \n      \n      self$hidden2() |>  \n      self$activation() |>  \n\n      self$hidden3() |>  \n      self$activation() |>  \n\n      self$output()\n  }\n)\n```\n:::\n\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn <- modelnn |> \n  setup(loss = nn_mse_loss(),\n        optimizer = optim_rmsprop) |>\n  set_hparams(input_size = ncol(px))\n```\n:::\n\n\n## Model Fitting\n\n::: {.panel-tabset}\n\n### Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- modelnn |> \n  fit(data = list(px, py), \n      epochs = 50)\n```\n:::\n\n\n### Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fitted)\n```\n:::\n\n\n:::\n\n\n# Training/Validating/Testing Data\n\n\n## Error Rate\n\nWhen creating a model, we are interested in determining how effective the model will be in predicting a new data point, ie not in our training data. \n\n::: {.fragment}\nThe error rate is a metric to determine how often willfuture data points be wrong when using our model.\n:::\n\n::: {.fragment}\nThe problem is how can we get future data to validate our model?\n:::\n\n## Training/Validating/Testing Data\n\nThe Training/Validating/Testing Data set is a way to take the original data set and split into 3 seperate data sets: training, validating, and testing.\n\n::: {.panel-tabset}\n\n### Training\n\nThis is data used to create the model. \n\n### Validating\n\nThis is data used to evaluate the data during it's creation. It evaluates at each iteration (Epoch)\n\n### Testing\n\nThis is data used to test the final model and compute the error rate.\n\n:::\n\n## Training/Validating/Testing Proportions\n\n::: {.columns}\n::: {.column}\n\n$$\n80/10/10\n$$\n\n\n:::\n::: {.column}\n\n$$\n70/15/15\n$$\n\n:::\n:::\n\n## Training Error Rate\n\nTraining Error Rate is the error rate of the data used to create the model of interest. It describes how well the model predicts the data used to construct it.\n\n## Test Error Rate\n\nTest Error Rate is the error rate of predicting a new data point using the current established model.\n\n\n\n\n## Penguin Data\n\n::: {.panel-tabset}\n\n### Train/Test/Evaluation\n  \n\n::: {.cell}\n\n```{.r .cell-code}\npenguins <- penguins |> drop_na()\ntraining <- penguins |> slice_sample(prop = .8)\npre <- penguins |> anti_join(training)\nvalidate <- pre |> slice_sample(prop =  0.5)\ntesting <- pre |> anti_join(validate)\n```\n:::\n\n\n\n\n###  Training\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtraining <- training |> \n  model.matrix(body_mass ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYtraining <- training |> \n  select(body_mass) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n\n### Validate\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXvalidate <- validate |> \n  model.matrix(body_mass ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYvalidate <- validate |> \n  select(body_mass) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n\n### Testing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtesting <- testing |> \n  model.matrix(body_mass ~ . - 1, data = _) |> \n  scale() |> \n  torch_tensor(dtype = torch_float())\n\nYtesting <- testing |> \n  select(body_mass) |> \n  as.matrix() |> \n  torch_tensor(dtype = torch_float())\n```\n:::\n\n\n:::\n\n## Model Description\n\n \n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn <- nn_module(\n  initialize = function(input_size) {\n    self$hidden1 <- nn_linear(in_features = input_size, \n                              out_features = 20)\n    self$hidden2 <- nn_linear(in_features = 20, \n                              out_features = 10)\n    self$hidden3 <- nn_linear(in_features = 10, \n                              out_features = 5)\n\n    self$output <- nn_linear(in_features = 5, \n                             out_features = 1)\n        \n    self$activation <- nn_relu()\n  },\n  forward = function(x) {\n    x |>  \n      self$hidden1() |>  \n      self$activation() |>  \n      \n      self$hidden2() |>  \n      self$activation() |>  \n\n      self$hidden3() |>  \n      self$activation() |>  \n\n      self$output()\n  }\n)\n```\n:::\n\n\n\n## Optimizer Set Up\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelnn <- modelnn |> \n  setup(\n    loss = nn_mse_loss(), # Used for numerical counts\n    optimizer = optim_rmsprop\n  ) |>\n  set_hparams(input_size = ncol(px))\n```\n:::\n\n\n\n## Fit a Model\n\n::: {.panel-tabset}\n## Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted <- modelnn |> \n  fit(\n    data = list(Xtraining, Ytraining),\n    epochs = 50, # Can think as number of iterations\n    valid_data = list(Xvalidate, Yvalidate)\n  )\n```\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(fitted)\n```\n:::\n\n\n\n## Testing Model\n\n::: {.panel-tabset}\n\n## Prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnpred <- predict(fitted, Xtesting)\n```\n:::\n\n\n## MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(abs(as.matrix(Ytesting) - as.matrix(npred)))\n```\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot(as.matrix(Ytesting), as.matrix(npred),\n     xlab = \"Truth\",\n     ylab = \"Predicted\")\n```\n:::\n\n\n:::\n\n\n# Thursday\n\n## Thursday\n\nCome perpared to work on your smart goal.\n\nShow evidence, either by submitting a word document, notebook, or other format, that you accomplished last week's smart goal.\n\n",
    "supporting": [
      "4a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}